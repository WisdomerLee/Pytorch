{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "14TensorFlow2_pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WisdomerLee/Pytorch/blob/main/14TensorFlow2_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPcaXml-vbhO"
      },
      "source": [
        "Tensorflow 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EomOZoFlvbhO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXADotfvbhO"
      },
      "source": [
        "Hyperparameter Tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dkFIGi1vbhO"
      },
      "source": [
        "num_epochs = 1\n",
        "batch_size = 64\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_shape= (28,28,1)\n",
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvC29_ZTvbhO"
      },
      "source": [
        "Preporcess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ARhATCQvbhO"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9NkPV1jvbhO"
      },
      "source": [
        "train_x = train_x[..., tf.newaxis]\n",
        "test_x = test_x[..., tf.newaxis]\n",
        "\n",
        "train_x= train_x/255.\n",
        "test_x= test_x/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqrN2QmjvbhO"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgeDSJWfvbhP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RXoAhPivbhP"
      },
      "source": [
        "inputs = layers.Input(input_shape)\n",
        "net = layers.Conv2D(32, (3,3), padding='SAME')(inputs)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.Conv2D(32, (3,3), padding= 'SAME')(net)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.MaxPool2D(pool_size=(2,2))(net)\n",
        "net = layers.Dropout(0.5)(net)\n",
        "\n",
        "net = layers.Conv2D(64, (3,3), padding='SAME')(inputs)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.Conv2D(64, (3,3), padding= 'SAME')(net)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.MaxPool2D(pool_size=(2,2))(net)\n",
        "net = layers.Dropout(0.5)(net)\n",
        "\n",
        "net = layers.Flatten()(net)\n",
        "net = layers.Dense(512)(net)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.Dropout(0.5)(net)\n",
        "net = layers.Dense(num_classes)(net)\n",
        "net = layers.Activation('softmax')(net)\n",
        "\n",
        "model = tf.keras.Model(inputs = inputs, outputs=net, name='Basic_CNN')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98JVKlYnvbhP"
      },
      "source": [
        "#\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), #optimization\n",
        "              loss='sparse_categorical_crossentropy', #loss function\n",
        "              metrics=['accuracy']) #Metrics/Accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMLdfQfNvbhP"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4EpCm5_vbhP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQcfvMxrvbhP",
        "outputId": "cc2e89aa-0301-46f8-f0a2-2c2e7ba8f328"
      },
      "source": [
        "model.fit(train_x, train_y,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=True)\n",
        "\n",
        "model.evaluate(test_x, test_y, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 93s 99ms/step - loss: 0.1545 - accuracy: 0.9520\n",
            "157/157 [==============================] - 3s 20ms/step - loss: 0.0421 - accuracy: 0.9856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.042110368609428406, 0.9855999946594238]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDmYd6SbvbhQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eexghSU-vbhQ"
      },
      "source": [
        "Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwL50-ImvbhQ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4pNTG52vbhQ"
      },
      "source": [
        "seed = 1\n",
        "\n",
        "lr = 0.001\n",
        "momentum=0.5\n",
        "\n",
        "batch_size = 64\n",
        "test_batch_size = 64\n",
        "\n",
        "epochs = 1\n",
        "no_cuda = False\n",
        "log_interval = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcTLvFG2vbhQ"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOCDyuQrvbhQ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1= nn.Conv2d(1,20, 5,1)\n",
        "        self.conv2 =nn.Conv2d(20,50,5,1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2,2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x,2,2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cRXURg2vbhQ"
      },
      "source": [
        "Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfh4LvQ1vbhQ"
      },
      "source": [
        "\n",
        "torch.manual_seed(seed)\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers':1, 'pin_memory': True} if use_cuda else{}\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train = True, download = True,\n",
        "                  transform=transforms.Compose([\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.1307,),(0.3081,))\n",
        "                  ])),\n",
        "    batch_size=batch_size, shuffle = True, **kwargs)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train = False, transform=transforms.Compose([\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.1307,),(0.3081,))\n",
        "                  ])),\n",
        "    batch_size=batch_size, shuffle = True, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIIXiPhuvbhQ"
      },
      "source": [
        "Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WT9f70ivbhQ"
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK59ypAPvbhQ"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb27QdDRvbhQ",
        "outputId": "888a7ec1-f73d-43c5-ba77-aa8aca297387"
      },
      "source": [
        "for epoch in range(1, epochs+1):\n",
        "    #TrainMode\n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() #back propagation 계산 전에 0으로 기울기 계산\n",
        "        output=model(data)\n",
        "        loss=F.nll_loss(output, target)\n",
        "        loss.backward() #계산한 기울기를 적용\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % log_interval ==0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:6f}'.format(\n",
        "            epoch, batch_idx*len(data), len(train_loader.dataset),\n",
        "            100.*batch_idx/len(train_loader), loss.item()))\n",
        "            \n",
        "    #TestMode\n",
        "    model.eval() #batch norm이나 dropout등을 train mode 변환\n",
        "    test_loss=0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): #autograd engine, backpropagation이나 gradient 계산을 꺼서 memory 사용량을 줄이고 속도를 올림\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss +=F.nll_loss(output, target, reduction='sum').item() #sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)#get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() #pred와 target이 같은지 확인\n",
        "            \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    print('\\n Test set: Average loss: {:.4f}, Accuracy:{}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset), 100.*correct/len(test_loader.dataset)))        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\t Loss: 2.300880\n",
            "Train Epoch: 1 [6400/60000 (11%)]\t Loss: 2.239080\n",
            "Train Epoch: 1 [12800/60000 (21%)]\t Loss: 2.136497\n",
            "Train Epoch: 1 [19200/60000 (32%)]\t Loss: 1.950306\n",
            "Train Epoch: 1 [25600/60000 (43%)]\t Loss: 1.701471\n",
            "Train Epoch: 1 [32000/60000 (53%)]\t Loss: 1.167160\n",
            "Train Epoch: 1 [38400/60000 (64%)]\t Loss: 0.817302\n",
            "Train Epoch: 1 [44800/60000 (75%)]\t Loss: 0.626805\n",
            "Train Epoch: 1 [51200/60000 (85%)]\t Loss: 0.475082\n",
            "Train Epoch: 1 [57600/60000 (96%)]\t Loss: 0.447327\n",
            "\n",
            " Test set: Average loss: 0.4469, Accuracy:8859/10000 (89%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}